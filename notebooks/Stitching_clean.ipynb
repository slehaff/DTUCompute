{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict,OrderedDict\n",
    "def stitching_new(\n",
    "                    #input \n",
    "                    source, \n",
    "                    target, \n",
    "                    labels= None,\n",
    "                    mytitle = None,\n",
    "    \n",
    "                    #add_outlier_removal\n",
    "                    add_outlier_removal = False,\n",
    "    \n",
    "                    #downsampling \n",
    "                    downsampling_voxel_size = 0.1,\n",
    "                    limit_size_point_cloud = 30000,\n",
    "                  \n",
    "                    #registration\n",
    "                    voxel_size = 0.2,\n",
    "                    mmax_iteration = 10**7,\n",
    "                    mmax_validation = 0.999,\n",
    "                  \n",
    "                    # quality control of the stitching\n",
    "                    myoverlapping_factor = 0.5,\n",
    "                    maxnumattempts = 10,\n",
    "                  \n",
    "                    # visualization and pringtng parameters\n",
    "                    trans_init = None, # keep the possibility of starting directly the ICP\n",
    "                    print_statements = False,\n",
    "                    save_statements = True,\n",
    "                    visualization_on = False,\n",
    "                    final_vis_on = True,\n",
    "                    params = myparams,  \n",
    "                    configuration_file = myconfiguration_file\n",
    "                 ):\n",
    "    \n",
    "    #labels\n",
    "    if labels == None:\n",
    "        labels = [\"source\",\"target\"]\n",
    "    # print title \n",
    "    if mytitle == None:\n",
    "        mytitle = f\"stitch_{labels[0]}_{labels[1]}\"\n",
    "    \n",
    "    print (mytitle)\n",
    "    \n",
    "    ## INITIAL VISUALIZATION \n",
    "    # Use different colors on the two point clouds\n",
    "    source.paint_uniform_color([1, 0.706, 0])     #source is yellow\n",
    "    target.paint_uniform_color([0, 0.651, 0.929]) #target is blue\n",
    "    # visualize\n",
    "    if  visualization_on == True: \n",
    "        custom_draw_geometry(source+target,\n",
    "                         mytitle = \"initial_\"+mytitle,\n",
    "                         params = params,  \n",
    "                         configuration_file = configuration_file, \n",
    "                         rotate = True)\n",
    "\n",
    "        \n",
    "    # count number of point in point clouds\n",
    "    points_source,points_target = get_num_points([source,target],print_statement = False)\n",
    "    \n",
    "    \n",
    "    #ADDITIONAL OUTLIER REMOVAL\n",
    "    #Outlier removal for two point clouds separately\n",
    "    if add_outlier_removal == True:\n",
    "        if print_statements == True:\n",
    "            print (\"\\noutlier removal\")\n",
    "        source, outlier_index_source = source.remove_radius_outlier(\n",
    "                                                      nb_points=16,\n",
    "                                                      radius=0.45)\n",
    "\n",
    "        target, outlier_index_target = target.remove_radius_outlier(\n",
    "                                                      nb_points=16,\n",
    "                                                      radius=0.5)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## DOWNSAMPLING BIG CLOUDS\n",
    "    # if pointclouds are bigger than limit_size_point_cloud then \n",
    "    if print_statements == True:\n",
    "        print (\"\\nDOWNSAMPLING BIG CLOUDS\")\n",
    "    processed_list, downsampling = downsample_big_clouds(\n",
    "                                                          [source,target],                     \n",
    "                                                          labels= labels,\n",
    "                                                          downsampling_voxel_size = downsampling_voxel_size,\n",
    "                                                          limit_size_point_cloud = limit_size_point_cloud,\n",
    "                                                          print_statements = print_statements,\n",
    "                                                          visualization_on = visualization_on\n",
    "                                                         )\n",
    "    processed_source, processed_target = processed_list \n",
    "    \n",
    "    # visualize if there has been any downsampling \n",
    "    if  visualization_on == True and downsampling == True: \n",
    "        custom_draw_geometry(processed_source+processed_target,\n",
    "                             mytitle = \"downsampled_\"+mytitle,\n",
    "                             params = params,  \n",
    "                             configuration_file = configuration_file, \n",
    "                             rotate = True,\n",
    "                             onewindow = True)\n",
    "    \n",
    "\n",
    "\n",
    "    # DATA PREPARATION\n",
    "    if print_statements == True:\n",
    "        print (\"\\nDATA PREPARATION\")\n",
    "    new_source, new_target, source_down, target_down, source_fpfh, target_fpfh = prepare_dataset_new(processed_source,\n",
    "                                                                                                     processed_target,\n",
    "                                                                                                     voxel_size, \n",
    "                                                                                                     mytitle =mytitle,\n",
    "                                                                                                     print_statements = print_statements\n",
    "                                                                                                      )\n",
    "\n",
    "    ## QUALITY STATEMENT \n",
    "    # initialization \n",
    "    overlapping_points = np.zeros(1) \n",
    "    numattempts = 0\n",
    "    \n",
    "    # quality condition\n",
    "    points_source,points_target = get_num_points([source,target],print_statement = False)\n",
    "    valuetoreach = myoverlapping_factor*(points_target)\n",
    "    if print_statements == True:\n",
    "        print (\"value to reach: \",valuetoreach)\n",
    "        print (f\"source has {points_source} points\")\n",
    "        print (f\"target has {points_target} points\")\n",
    "    quality_needs_improvement = (len(np.asarray(overlapping_points)) < valuetoreach)\n",
    "    \n",
    "    #create dict of all results\n",
    "    all_results = defaultdict (list)\n",
    "    \n",
    "    ## while the quality condition or the max number of attempts is not satisfied\n",
    "    # either we get a stitch of the quality that we want in terms of overlapping points\n",
    "    # either we try a number of times and take the best in terms of overlapping points\n",
    "    while (numattempts <= maxnumattempts-1) and (quality_needs_improvement):\n",
    "        numattempts += 1\n",
    "        \n",
    "        if print_statements == True:\n",
    "            print (\"-\"*100)\n",
    "            print (f\"ATTEMPT {numattempts}\")\n",
    "        \n",
    "        this_attempt_list_results = list()\n",
    "    \n",
    "        #GLOBAL REGISTRATION: execute ransac\n",
    "        result_ransac = execute_global_registration(source_down, target_down,\n",
    "                                                    source_fpfh, target_fpfh,\n",
    "                                                    voxel_size= voxel_size,\n",
    "                                                    max_iteration = mmax_iteration,\n",
    "                                                    max_validation = mmax_validation,\n",
    "                                                    print_statements = print_statements\n",
    "                                                   )\n",
    "        #print result\n",
    "        if print_statements == True:\n",
    "            print (result_ransac)\n",
    "            print(\"Transformation is:\")\n",
    "            print(result_ransac.transformation)\n",
    "        #visualize\n",
    "        if visualization_on == True: \n",
    "            draw_registration_result(source, target, \n",
    "                                     result_ransac.transformation,\n",
    "                                     title = \"global registration-%s\"%mytitle,\n",
    "                                    )\n",
    "        # append the just obtained one in this_attempt_list_results\n",
    "        this_attempt_list_results.append(result_ransac)\n",
    "\n",
    "                \n",
    "    \n",
    "\n",
    "        #LOCAL REGISTRATION: execute icp\n",
    "        result_icp = refine_registration(source, target, \n",
    "                                         source_fpfh, target_fpfh,\n",
    "                                         voxel_size= voxel_size,\n",
    "                                         mytranformation =this_attempt_list_results[-1].transformation,\n",
    "                                         print_statements = print_statements\n",
    "                                        )\n",
    "        \n",
    "        points_source,points_target = get_num_points([source,target],print_statement = print_statements)\n",
    "        overlapping_points = result_icp.correspondence_set\n",
    "        quality_needs_improvement = (len(np.asarray(overlapping_points)) < valuetoreach)\n",
    "        \n",
    "        # print result\n",
    "        if print_statements == True:\n",
    "            print(result_icp)\n",
    "            print(\"Transformation is:\")\n",
    "            print(result_icp.transformation)\n",
    "            print (f\"Quality of the stitch needs improvment: {quality_needs_improvement}\")\n",
    "            \n",
    "       #append the just obtained one in this_attempt_list_results\n",
    "        this_attempt_list_results.append(result_icp)\n",
    "\n",
    "        \n",
    "        #add the two new registration results to the dictionary of attempts\n",
    "        all_results[numattempts] = this_attempt_list_results\n",
    "            \n",
    "    \n",
    "    if quality_needs_improvement == False:\n",
    "        if print_statements == True:\n",
    "            print (\"\\nRESULT: Built stitch of expected quality\")\n",
    "        best_transformation = all_results[numattempts][1].transformation\n",
    "        \n",
    "        \n",
    "    else: \n",
    "        ## sort results stored in the dictionary by the value of overlapping points        \n",
    "        index_best_attempt,best_registration_icp,best_transformation,list_all_results_ordered= extract_best_result_and_transformation(all_results)\n",
    "        \n",
    "        \n",
    "    if final_vis_on == True: \n",
    "        draw_registration_result(source, target, \n",
    "                                 best_transformation,\n",
    "                                 title = \"last fit registration-%s\"%mytitle\n",
    "                                )\n",
    "    \n",
    "    #build the final newpointcloud and eventually save it\n",
    "    newpointcloud = save_registration_result(source, target, best_transformation, \n",
    "                                             mytitle, \n",
    "                                             save_result = save_statements,\n",
    "                                             visualize_result = False)\n",
    "        \n",
    "    \n",
    "    return new_source, new_target, newpointcloud ,all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_point_cloud(pcd_down, voxel_size,pprint_statements = False):\n",
    "    \n",
    "    #pcd_down = pcd.voxel_down_sample(voxel_size)\n",
    "    radius_normal = voxel_size * 2\n",
    "    radius_feature = voxel_size * 5\n",
    "    \n",
    "    pcd_down.estimate_normals(\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius_normal, max_nn=230))\n",
    "    \n",
    "    pcd_fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "        pcd_down,\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius_feature, max_nn=230))\n",
    "    \n",
    "    if pprint_statements== True: \n",
    "        print(\"\\n Downsample with a voxel size %.3f.\" % voxel_size)\n",
    "        print(\"Estimate normal with search radius %.3f.\" % radius_normal)\n",
    "        print(\"Compute FPFH feature with search radius %.3f.\" % radius_feature)\n",
    "\n",
    "    return pcd_down, pcd_fpfh\n",
    "\n",
    "def prepare_dataset(source,target,voxel_size = None,mytitle = \"\", print_statements = False):\n",
    "    \"\"\"\n",
    "    source and target are already downsampled \n",
    "    \"\"\"\n",
    "\n",
    "    source_down, source_fpfh = preprocess_point_cloud(source, voxel_size)\n",
    "    target_down, target_fpfh = preprocess_point_cloud(target, voxel_size)\n",
    "\n",
    "    \n",
    "    #outlier removal\n",
    "    if print_statements== True:\n",
    "        print (\"removing outliers\")\n",
    "    processed_source, outlier_index = source.remove_radius_outlier(\n",
    "                                              nb_points=25,\n",
    "                                              radius=0.5)\n",
    "\n",
    "    processed_target, outlier_index = target.remove_radius_outlier(\n",
    "                                              nb_points=25,\n",
    "                                              radius=0.5)\n",
    "\n",
    "    return source, target, source_down, target_down, source_fpfh, target_fpfh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global and icp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_global_registration(source_down, target_down, source_fpfh,\n",
    "                                target_fpfh, voxel_size,\n",
    "                                max_iteration,\n",
    "                                max_validation,\n",
    "                                print_statements = False,\n",
    "                               ):\n",
    "    distance_threshold = voxel_size *1.5\n",
    "    \n",
    "    if print_statements== True: \n",
    "        print(\"\\nGLOBAL REGISTRATION: RANSAC registration on downsampled point clouds.\")\n",
    "        #print(\"   Since the downsampling voxel size is %.3f,\" % voxel_size)\n",
    "        #print(\"   we use a liberal dista¢nce threshold %.3f.\" % distance_threshold)\n",
    "    result = o3d.pipelines.registration.registration_ransac_based_on_feature_matching(\n",
    "        source_down, target_down, source_fpfh, target_fpfh, True,\n",
    "        distance_threshold,\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPoint(False),\n",
    "        3, [\n",
    "            o3d.pipelines.registration.CorrespondenceCheckerBasedOnEdgeLength(\n",
    "                0.99),\n",
    "            o3d.pipelines.registration.CorrespondenceCheckerBasedOnDistance(\n",
    "                distance_threshold)\n",
    "        ], o3d.pipelines.registration.RANSACConvergenceCriteria(max_iteration,max_validation))\n",
    "    \n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "#icp\n",
    "def refine_registration(source, target, \n",
    "                        source_fpfh, target_fpfh, \n",
    "                        voxel_size,\n",
    "                        mytranformation = None,\n",
    "                        print_statements = False\n",
    "                       ):\n",
    "    \n",
    "    distance_threshold = voxel_size *2\n",
    "    \n",
    "    if print_statements== True: \n",
    "        print(\"\\nPOINT-TO-PLANE ICP registration is applied on original point\")\n",
    "        print(\"distance threshold %.3f.\" % distance_threshold)\n",
    "        \n",
    "    #if type(mytranformation) != \"numpy.ndarray\":\n",
    "        #mytranformation = np.identity(4)\n",
    "        \n",
    "    \n",
    "    radius_normal = voxel_size * 5\n",
    "    \n",
    "    source.estimate_normals(\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius_normal, max_nn=230))\n",
    "    target.estimate_normals(\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius_normal, max_nn=230))\n",
    "    \n",
    "    result = o3d.pipelines.registration.registration_icp(\n",
    "        source, target, distance_threshold, mytranformation,\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPlane())\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_points(list_pointclouds, print_statement = False):\n",
    "    num_points = []\n",
    "    n_pc = list(range(len(list_pointclouds)))\n",
    "    for i in n_pc:\n",
    "        points = len(np.asarray(list_pointclouds[i].points))\n",
    "        num_points.append(points)\n",
    "    #print(\"\")\n",
    "    if print_statement == True:\n",
    "        #print(\"\")\n",
    "        print(\"number of points in clouds\")\n",
    "        print (*zip(n_pc,num_points), sep = \"\\n\")\n",
    "    return num_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_registration_result(source, target, transformation,\n",
    "                             \n",
    "                             #visualization parameters\n",
    "                             title = \"\", mytuples = None, \n",
    "                             params = None, #camera parameters,json file (P)\n",
    "                             fov_step  = None, \n",
    "                             configuration_file = None, #object properties ,json file (O)\n",
    "                             rotate = False,\n",
    "                             \n",
    "                             save_result = True,\n",
    "                             visualize_result = False):\n",
    "    \n",
    "    dt_string = mytimestamp()\n",
    "    filename = dt_string+'-stitch_'+title+'.pcd'\n",
    "    \n",
    "    # apply the chosen transformation to source and target\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    target_temp = copy.deepcopy(target)\n",
    "    source_temp.paint_uniform_color([1, 0.706, 0])\n",
    "    target_temp.paint_uniform_color([0, 0.651, 0.929])\n",
    "    source_temp.transform(transformation)\n",
    "    \n",
    "    # combine them and create the newpoint cloud\n",
    "    newpointcloud = source_temp + target_temp\n",
    "    #newpointcloud.paint_uniform_color([0,0.5,0.1])\n",
    "    \n",
    "    #save\n",
    "    if save_result == True: \n",
    "        o3d.io.write_point_cloud(filename, newpointcloud)\n",
    "    \n",
    "    #visualize\n",
    "    if visualize_result == True:\n",
    "        \n",
    "        custom_draw_geometry(newpointcloud,\n",
    "                     mytitle = title,\n",
    "                     params = params,  # parameter for camera point view, json file via pressing P\n",
    "                     configuration_file = configuration_file, # configuration file for properties, json file via pressing o\n",
    "                     rotate = rotate\n",
    "                            )\n",
    "        \n",
    "        \n",
    "        \n",
    "    return newpointcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_best_result_and_transformation(all_results):\n",
    "    \n",
    "    all_results_ordered= OrderedDict(sorted(all_results.items (),key=lambda t: len(np.asarray(t[1][1].correspondence_set))))\n",
    "    # make it into a list\n",
    "    list_all_results_ordered= list ((k,v) for k,v in all_results_ordered.items())\n",
    "    # take the last one (with the highest correspondence set)\n",
    "    best_result = list_all_results_ordered[-1]\n",
    "    # get the registration out\n",
    "    index_best_attempt = best_result[0]\n",
    "    best_registration_icp = best_result[1][1]\n",
    "    best_transformation = best_registration_icp.transformation \n",
    "    return index_best_attempt,best_registration_icp,best_transformation,list_all_results_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
